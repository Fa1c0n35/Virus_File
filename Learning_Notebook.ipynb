{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2dSyboSFRR7",
    "outputId": "8da10d6f-d2a2-4ee0-919f-bce5171ec8f3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import sklearn.ensemble as ske \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree, linear_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Hp1p2lOkFWh6"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', sep='|') #generate df as data\n",
    "X = data.drop(['Name', 'md5', 'legitimate'], axis=1).values #now droping some coloumns as axis 1(mean coloumn) and will show the values in the rows\n",
    "y = data['legitimate'].values #values of legitimate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-FfJbUM_Fok3",
    "outputId": "2cb57836-408f-4adc-a02a-39945d5a4830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Researching important feature based on 54 total features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Researching important feature based on %i total features\\n' % X.shape[1])# shape() is use in pandas to give number of row/column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn: Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection using Trees Classifier\n",
    "#### - ExtraTreesClassifier\n",
    "#### - RandomForestClassifier\n",
    "#### - DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fsel = ske.ExtraTreesClassifier().fit(X, y)\n",
    "#fsel = ske.RandomForestClassifier().fit(X, y)\n",
    "#fsel = DecisionTreeClassifier().fit(X, y)\n",
    "#model = SelectFromModel(fsel, prefit=True)\n",
    "#X_new = model.transform(X)\n",
    "#nb_features = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1L8yAxiuInm_"
   },
   "outputs": [],
   "source": [
    "fsel = ske.ExtraTreesClassifier(n_estimators=250, random_state=7).fit(X, y)\n",
    "#fsel = ske.RandomForestClassifier(n_estimators=250, random_state=7).fit(X, y)\n",
    "#fsel = DecisionTreeClassifier(random_state=7).fit(X, y)\n",
    "model = SelectFromModel(fsel, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "nb_features = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "09c-w3X6Iqjz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y ,test_size=0.2)#now converting in training and testing data in 20% range hahhahaha ! as total x is 138047 and testing is 138047*0.2=27610 :)\n",
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8_9xscMIxRe",
    "outputId": "e053fe2c-b630-4ca0-e5bb-262912229e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 features identified as important:\n"
     ]
    }
   ],
   "source": [
    "print('%i features identified as important:' % nb_features) #as mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZE4HPHuXI6ys",
    "outputId": "a041b671-52c7-4d97-8e54-f656bff24c74",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. feature DllCharacteristics (0.145156)\n",
      "2. feature Characteristics (0.107261)\n",
      "3. feature Machine (0.099439)\n",
      "4. feature SectionsMaxEntropy (0.067718)\n",
      "5. feature Subsystem (0.062970)\n",
      "6. feature VersionInformationSize (0.058362)\n",
      "7. feature ImageBase (0.057088)\n",
      "8. feature ResourcesMaxEntropy (0.049035)\n",
      "9. feature MajorSubsystemVersion (0.046935)\n",
      "10. feature SizeOfOptionalHeader (0.037471)\n",
      "11. feature ResourcesMinEntropy (0.030245)\n",
      "12. feature SizeOfStackReserve (0.027731)\n",
      "13. feature SectionsMinEntropy (0.020134)\n",
      "14. feature MajorOperatingSystemVersion (0.019677)\n"
     ]
    }
   ],
   "source": [
    "#important features sored\n",
    "indices = np.argsort(fsel.feature_importances_)[::-1][:nb_features]\n",
    "for f in range(nb_features):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, data.columns[2+indices[f]], fsel.feature_importances_[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "9R0SCMRrI_WL"
   },
   "outputs": [],
   "source": [
    "# mean adding to the empty 'features' array the 'important features'\n",
    "for f in sorted(np.argsort(fsel.feature_importances_)[::-1][:nb_features]):#[::-1] mean start with last towards first \n",
    "    features.append(data.columns[2+f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm comparison\n",
    "#algorithms = {\n",
    "        #\"DecisionTree\": tree.DecisionTreeClassifier(max_depth=10),\n",
    "    \n",
    "        #\"RandomForest\": ske.RandomForestClassifier(n_estimators=50),.\n",
    "    \n",
    "        #\"GradientBoosting\": ske.GradientBoostingClassifier(n_estimators=50),\n",
    "        #\"AdaBoost\": ske.AdaBoostClassifier(n_estimators=100),\n",
    "        \n",
    "        #\"GNB\": GaussianNB()\n",
    "    #}\n",
    "\n",
    "#results = {}\n",
    "#print(\"\\nNow testing algorithms\")\n",
    "#for algo in algorithms:\n",
    "#    clf = algorithms[algo]\n",
    "#    clf.fit(X_train, y_train)\n",
    "#    score = clf.score(X_test, y_test)\n",
    "#    print(\"%s : %f %%\" % (algo, score*100))\n",
    "#    results[algo] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now testing algorithms\n",
      "KNeighborsClassifier score: 0.9898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAB02\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression   score: 0.7026\n",
      "Perceptron           score: 0.2971\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 90.9 GiB for an array with shape (110437, 110437) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6417e2f4e82d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mscore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    655\u001b[0m                                  % self.multi_class)\n\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_marginal_likelihood_value_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_marginal_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;31m# Precompute quantities required for predictions which are independent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpc.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;31m# Compute log-marginal-likelihood Z and also store some temporaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m    855\u001b[0m                                        K2_gradient * K1[:, :, np.newaxis]))\n\u001b[0;32m    856\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[0;32m   1158\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Gradient can only be evaluated when Y is None.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m         K = np.full((_num_samples(X), _num_samples(Y)), self.constant_value,\n\u001b[0m\u001b[0;32m   1161\u001b[0m                     dtype=np.array(self.constant_value).dtype)\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 90.9 GiB for an array with shape (110437, 110437) and data type float64"
     ]
    }
   ],
   "source": [
    "algo=[\n",
    "    [KNeighborsClassifier(n_neighbors=5), 'KNeighborsClassifier'], \n",
    "    [LogisticRegression(solver='lbfgs'), 'LogisticRegression'], \n",
    "    [Perceptron(), 'Perceptron'],\n",
    "    #[DecisionTreeClassifier(min_samples_split=10), 'DecisionTreeClassifier'],\n",
    "    #[GradientBoostingClassifier(), 'GradientBoostingClassifier'],\n",
    "    #[RandomForestClassifier(), 'RandomForestClassifier'],\n",
    "    #[BaggingClassifier(), 'BaggingClassifier'],\n",
    "    #[AdaBoostClassifier(), 'AdaBoostClassifier'],\n",
    "    #[GaussianNB(), 'GaussianNB'],\n",
    "    #[MLPClassifier(), 'MLPClassifier'],\n",
    "    #[SVC(kernel='linear'), 'SVC_linear'], \n",
    "    [GaussianProcessClassifier(), 'GaussianProcessClassifier']\n",
    "]\n",
    "model_scores=[]\n",
    "print(\"\\nNow testing algorithms\")\n",
    "for a in algo:\n",
    "    model = a[0]\n",
    "    model.fit(X_train, y_train)\n",
    "    score=model.score(X_test, y_test)\n",
    "    model_scores.append([score, a[1]])\n",
    "    y_pred=model.predict(X_test)\n",
    "    print(f'{a[1]:20} score: {score:.04f}')\n",
    "    #print(metrics.confusion_matrix(y_test, y_pred))\n",
    "    #print(metrics.classification_report(y_test, y_pred))\n",
    "    #print('-' * 100)\n",
    "\n",
    "print(model_scores)\n",
    "print(f'best score = {max(model_scores)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "6DKQefCOJIpH",
    "outputId": "91d7ef0a-9bdb-4667-d3d9-fb38aac97e6e"
   },
   "outputs": [],
   "source": [
    "winner = max(results, key=results.get)\n",
    "print('\\nWinner algorithm is %s with a %f %% success' % (winner, results[winner]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "cA-Zn5BUJwtR",
    "outputId": "b2269b5e-b0f4-4237-dd19-3d6177f283c3"
   },
   "outputs": [],
   "source": [
    "# Save the algorithm and the feature list for later predictions\n",
    "print('Saving algorithm and feature list in classifier directory...')\n",
    "joblib.dump(algorithms[winner], 'classifier/classifier.pkl')\n",
    "open('classifier/features.pkl', 'wb').write(pickle.dumps(features))\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "RgBDOa0zJ4Ft",
    "outputId": "34fca9fe-73d3-46db-dd46-2578e14b24b3"
   },
   "outputs": [],
   "source": [
    "# Identify false and true positive rates\n",
    "clf = algorithms[winner]\n",
    "res = clf.predict(X_test)\n",
    "mt = confusion_matrix(y_test, res)\n",
    "print(\"False positive rate : %f %%\" % ((mt[0][1] / float(sum(mt[0])))*100))\n",
    "print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIbrsaEdK4lP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Learning_Notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
